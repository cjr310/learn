{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjr310/learn/blob/master/colab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq67gSpYxTta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhhJBKMxxpLd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5394
        },
        "outputId": "52b3cfab-7d2d-4826-d2c0-d35b6e6f2869"
      },
      "source": [
        "import  torch\n",
        "from    torch import  nn\n",
        "from    torch.nn import functional as F\n",
        "from    torch.utils.data import DataLoader\n",
        "from    torchvision import datasets\n",
        "from    torchvision import transforms\n",
        "from    torch import nn, optim\n",
        "\n",
        "# from    torchvision.models import resnet18\n",
        "\n",
        "class ResBlk(nn.Module):\n",
        "    \"\"\"\n",
        "    resnet block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        \"\"\"\n",
        "        :param ch_in:\n",
        "        :param ch_out:\n",
        "        \"\"\"\n",
        "        super(ResBlk, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(ch_out)\n",
        "        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(ch_out)\n",
        "\n",
        "        self.extra = nn.Sequential()\n",
        "        if ch_out != ch_in:\n",
        "            # [b, ch_in, h, w] => [b, ch_out, h, w]\n",
        "            self.extra = nn.Sequential(\n",
        "                nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1),\n",
        "                nn.BatchNorm2d(ch_out)\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"-\n",
        "        :param x: [b, ch, h, w]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        # short cut.\n",
        "        # extra module: [b, ch_in, h, w] => [b, ch_out, h, w]\n",
        "        # element-wise add:\n",
        "        out = self.extra(x) + out\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ResNet18, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16)\n",
        "        )\n",
        "        # followed 4 blocks\n",
        "        # [b, 64, h, w] => [b, 128, h ,w]\n",
        "        self.blk1 = ResBlk(16, 16)\n",
        "        # [b, 128, h, w] => [b, 256, h, w]\n",
        "        self.blk2 = ResBlk(16, 32)\n",
        "        # # [b, 256, h, w] => [b, 512, h, w]\n",
        "        # self.blk3 = ResBlk(128, 256)\n",
        "        # # [b, 512, h, w] => [b, 1024, h, w]\n",
        "        # self.blk4 = ResBlk(256, 512)\n",
        "\n",
        "        self.outlayer = nn.Linear(32*32*32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        x = F.relu(self.conv1(x))\n",
        "\n",
        "        # [b, 64, h, w] => [b, 1024, h, w]\n",
        "        x = self.blk1(x)\n",
        "        x = self.blk2(x)\n",
        "        # x = self.blk3(x)\n",
        "        # x = self.blk4(x)\n",
        "\n",
        "        # print(x.shape)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.outlayer(x)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    batchsz = 32\n",
        "\n",
        "    cifar_train = datasets.CIFAR10('cifar', True, transform=transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor()\n",
        "    ]), download=True)\n",
        "    cifar_train = DataLoader(cifar_train, batch_size=batchsz, shuffle=True)\n",
        "\n",
        "    cifar_test = datasets.CIFAR10('cifar', False, transform=transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor()\n",
        "    ]), download=True)\n",
        "    cifar_test = DataLoader(cifar_test, batch_size=batchsz, shuffle=True)\n",
        "\n",
        "\n",
        "    x, label = iter(cifar_train).next()\n",
        "    print('x:', x.shape, 'label:', label.shape)\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "    # model = Lenet5().to(device)\n",
        "    model = ResNet18().to(device)\n",
        "\n",
        "    criteon = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    print(model)\n",
        "\n",
        "    for epoch in range(1000):\n",
        "\n",
        "        model.train()\n",
        "        for batchidx, (x, label) in enumerate(cifar_train):\n",
        "            # [b, 3, 32, 32]\n",
        "            # [b]\n",
        "            x, label = x.to(device), label.to(device)\n",
        "\n",
        "            logits = model(x)\n",
        "            # logits: [b, 10]\n",
        "            # label:  [b]\n",
        "            # loss: tensor scalar\n",
        "            loss = criteon(logits, label)\n",
        "\n",
        "            # backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        #\n",
        "        print(epoch, 'loss:', loss.item())\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # test\n",
        "            total_correct = 0\n",
        "            total_num = 0\n",
        "            for x, label in cifar_test:\n",
        "                # [b, 3, 32, 32]\n",
        "                # [b]\n",
        "                x, label = x.to(device), label.to(device)\n",
        "\n",
        "                # [b, 10]\n",
        "                logits = model(x)\n",
        "                # [b]\n",
        "                pred = logits.argmax(dim=1)\n",
        "                # [b] vs [b] => scalar tensor\n",
        "                correct = torch.eq(pred, label).float().sum().item()\n",
        "                total_correct += correct\n",
        "                total_num += x.size(0)\n",
        "                # print(correct)\n",
        "\n",
        "            acc = total_correct / total_num\n",
        "            print(epoch, 'acc:', acc)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 170156032/170498071 [00:27<00:00, 5233482.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "x: torch.Size([32, 3, 32, 32]) label: torch.Size([32])\n",
            "ResNet18(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (blk1): ResBlk(\n",
            "    (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (extra): Sequential()\n",
            "  )\n",
            "  (blk2): ResBlk(\n",
            "    (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (extra): Sequential(\n",
            "      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (outlayer): Linear(in_features=32768, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r170500096it [00:40, 5233482.50it/s]                               "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 loss: 0.8886869549751282\n",
            "0 acc: 0.5743\n",
            "1 loss: 0.5509520769119263\n",
            "1 acc: 0.5748\n",
            "2 loss: 0.5188194513320923\n",
            "2 acc: 0.6145\n",
            "3 loss: 0.2488974630832672\n",
            "3 acc: 0.6257\n",
            "4 loss: 0.8379215598106384\n",
            "4 acc: 0.6001\n",
            "5 loss: 0.5829387307167053\n",
            "5 acc: 0.6291\n",
            "6 loss: 0.29148155450820923\n",
            "6 acc: 0.6181\n",
            "7 loss: 0.4803783893585205\n",
            "7 acc: 0.5994\n",
            "8 loss: 0.17306968569755554\n",
            "8 acc: 0.5947\n",
            "9 loss: 0.5440123677253723\n",
            "9 acc: 0.6205\n",
            "10 loss: 0.7324976325035095\n",
            "10 acc: 0.6112\n",
            "11 loss: 0.5675333738327026\n",
            "11 acc: 0.6143\n",
            "12 loss: 0.3951891362667084\n",
            "12 acc: 0.6095\n",
            "13 loss: 0.031334102153778076\n",
            "13 acc: 0.6103\n",
            "14 loss: 0.06927216053009033\n",
            "14 acc: 0.6036\n",
            "15 loss: 0.1669706106185913\n",
            "15 acc: 0.6212\n",
            "16 loss: 0.3545164465904236\n",
            "16 acc: 0.601\n",
            "17 loss: 0.36397677659988403\n",
            "17 acc: 0.5912\n",
            "18 loss: 0.38776612281799316\n",
            "18 acc: 0.6115\n",
            "19 loss: 0.07959884405136108\n",
            "19 acc: 0.5961\n",
            "20 loss: 0.06300202012062073\n",
            "20 acc: 0.6169\n",
            "21 loss: 0.010641276836395264\n",
            "21 acc: 0.6001\n",
            "22 loss: 0.0035139918327331543\n",
            "22 acc: 0.611\n",
            "23 loss: 0.15889137983322144\n",
            "23 acc: 0.6039\n",
            "24 loss: 0.1469060182571411\n",
            "24 acc: 0.6088\n",
            "25 loss: 0.06803375482559204\n",
            "25 acc: 0.606\n",
            "26 loss: 0.004693150520324707\n",
            "26 acc: 0.599\n",
            "27 loss: 0.0766175389289856\n",
            "27 acc: 0.6104\n",
            "28 loss: 0.036337852478027344\n",
            "28 acc: 0.6063\n",
            "29 loss: 0.2045881152153015\n",
            "29 acc: 0.6036\n",
            "30 loss: 0.015076696872711182\n",
            "30 acc: 0.6039\n",
            "31 loss: 0.005674540996551514\n",
            "31 acc: 0.6083\n",
            "32 loss: 0.016705572605133057\n",
            "32 acc: 0.6078\n",
            "33 loss: 0.173420250415802\n",
            "33 acc: 0.5913\n",
            "34 loss: 0.4705545902252197\n",
            "34 acc: 0.6037\n",
            "35 loss: 0.027988195419311523\n",
            "35 acc: 0.6047\n",
            "36 loss: 0.20330268144607544\n",
            "36 acc: 0.6064\n",
            "37 loss: 0.15510666370391846\n",
            "37 acc: 0.6037\n",
            "38 loss: 0.14836347103118896\n",
            "38 acc: 0.6016\n",
            "39 loss: 0.04694342613220215\n",
            "39 acc: 0.605\n",
            "40 loss: 0.09292888641357422\n",
            "40 acc: 0.6067\n",
            "41 loss: 0.023352444171905518\n",
            "41 acc: 0.6046\n",
            "42 loss: 0.0007743239402770996\n",
            "42 acc: 0.6052\n",
            "43 loss: 0.001866459846496582\n",
            "43 acc: 0.6062\n",
            "44 loss: 7.784366607666016e-05\n",
            "44 acc: 0.5955\n",
            "45 loss: 0.18011528253555298\n",
            "45 acc: 0.5976\n",
            "46 loss: 0.007805228233337402\n",
            "46 acc: 0.6089\n",
            "47 loss: 0.07114160060882568\n",
            "47 acc: 0.6045\n",
            "48 loss: 0.002498149871826172\n",
            "48 acc: 0.5896\n",
            "49 loss: 0.18706679344177246\n",
            "49 acc: 0.6055\n",
            "50 loss: 0.17424482107162476\n",
            "50 acc: 0.6004\n",
            "51 loss: 0.001001596450805664\n",
            "51 acc: 0.614\n",
            "52 loss: 0.33250224590301514\n",
            "52 acc: 0.6057\n",
            "53 loss: 0.019830644130706787\n",
            "53 acc: 0.6066\n",
            "54 loss: 0.04777902364730835\n",
            "54 acc: 0.6013\n",
            "55 loss: 0.0017388463020324707\n",
            "55 acc: 0.6006\n",
            "56 loss: 0.00024169683456420898\n",
            "56 acc: 0.5978\n",
            "57 loss: 0.0007340908050537109\n",
            "57 acc: 0.6027\n",
            "58 loss: 4.7206878662109375e-05\n",
            "58 acc: 0.6073\n",
            "59 loss: 0.19752293825149536\n",
            "59 acc: 0.6023\n",
            "60 loss: 0.008617162704467773\n",
            "60 acc: 0.6123\n",
            "61 loss: 0.004286766052246094\n",
            "61 acc: 0.5987\n",
            "62 loss: 0.0001615285873413086\n",
            "62 acc: 0.6027\n",
            "63 loss: 0.2750743627548218\n",
            "63 acc: 0.6027\n",
            "64 loss: 0.002893209457397461\n",
            "64 acc: 0.6052\n",
            "65 loss: 0.025254487991333008\n",
            "65 acc: 0.601\n",
            "66 loss: 0.19924622774124146\n",
            "66 acc: 0.6055\n",
            "67 loss: 0.0010721683502197266\n",
            "67 acc: 0.5992\n",
            "68 loss: 0.0038259029388427734\n",
            "68 acc: 0.5961\n",
            "69 loss: 0.09359019994735718\n",
            "69 acc: 0.6112\n",
            "70 loss: 0.00010764598846435547\n",
            "70 acc: 0.6091\n",
            "71 loss: 0.005082130432128906\n",
            "71 acc: 0.6065\n",
            "72 loss: 0.004936933517456055\n",
            "72 acc: 0.6097\n",
            "73 loss: 0.00484853982925415\n",
            "73 acc: 0.5982\n",
            "74 loss: 0.35943603515625\n",
            "74 acc: 0.6062\n",
            "75 loss: 0.0006582736968994141\n",
            "75 acc: 0.5972\n",
            "76 loss: 0.002415299415588379\n",
            "76 acc: 0.6045\n",
            "77 loss: 0.008136153221130371\n",
            "77 acc: 0.5949\n",
            "78 loss: 0.021330952644348145\n",
            "78 acc: 0.6058\n",
            "79 loss: 0.30397915840148926\n",
            "79 acc: 0.6046\n",
            "80 loss: 2.09808349609375e-05\n",
            "80 acc: 0.604\n",
            "81 loss: 0.0016776323318481445\n",
            "81 acc: 0.6049\n",
            "82 loss: 0.04906153678894043\n",
            "82 acc: 0.6047\n",
            "83 loss: 0.08537453413009644\n",
            "83 acc: 0.6021\n",
            "84 loss: 0.00807720422744751\n",
            "84 acc: 0.6018\n",
            "85 loss: 0.0028204917907714844\n",
            "85 acc: 0.6121\n",
            "86 loss: 0.0023021697998046875\n",
            "86 acc: 0.6004\n",
            "87 loss: 0.000983595848083496\n",
            "87 acc: 0.6011\n",
            "88 loss: 0.016861796379089355\n",
            "88 acc: 0.6065\n",
            "89 loss: 0.2783820629119873\n",
            "89 acc: 0.6063\n",
            "90 loss: 0.14517617225646973\n",
            "90 acc: 0.6035\n",
            "91 loss: 0.009515762329101562\n",
            "91 acc: 0.5965\n",
            "92 loss: 0.00025534629821777344\n",
            "92 acc: 0.6083\n",
            "93 loss: 0.20333409309387207\n",
            "93 acc: 0.5983\n",
            "94 loss: 9.059906005859375e-06\n",
            "94 acc: 0.595\n",
            "95 loss: 9.107589721679688e-05\n",
            "95 acc: 0.5968\n",
            "96 loss: 0.0929555892944336\n",
            "96 acc: 0.6078\n",
            "97 loss: 0.0841672420501709\n",
            "97 acc: 0.6064\n",
            "98 loss: 0.09590744972229004\n",
            "98 acc: 0.5974\n",
            "99 loss: 0.00029647350311279297\n",
            "99 acc: 0.6063\n",
            "100 loss: 0.001124262809753418\n",
            "100 acc: 0.5918\n",
            "101 loss: 0.05315542221069336\n",
            "101 acc: 0.6022\n",
            "102 loss: 9.834766387939453e-05\n",
            "102 acc: 0.6033\n",
            "103 loss: 0.0003783702850341797\n",
            "103 acc: 0.6061\n",
            "104 loss: 1.4781951904296875e-05\n",
            "104 acc: 0.6083\n",
            "105 loss: 0.0013004541397094727\n",
            "105 acc: 0.6081\n",
            "106 loss: 0.7715438604354858\n",
            "106 acc: 0.6007\n",
            "107 loss: 0.0011153221130371094\n",
            "107 acc: 0.6026\n",
            "108 loss: 0.00020611286163330078\n",
            "108 acc: 0.6029\n",
            "109 loss: 0.06357622146606445\n",
            "109 acc: 0.6013\n",
            "110 loss: 0.00035309791564941406\n",
            "110 acc: 0.6012\n",
            "111 loss: 3.337860107421875e-06\n",
            "111 acc: 0.6005\n",
            "112 loss: 1.2159347534179688e-05\n",
            "112 acc: 0.5928\n",
            "113 loss: 9.334087371826172e-05\n",
            "113 acc: 0.5998\n",
            "114 loss: 1.0013580322265625e-05\n",
            "114 acc: 0.6057\n",
            "115 loss: 0.02346014976501465\n",
            "115 acc: 0.6022\n",
            "116 loss: 0.0002124309539794922\n",
            "116 acc: 0.5924\n",
            "117 loss: 7.092952728271484e-05\n",
            "117 acc: 0.5991\n",
            "118 loss: 7.152557373046875e-07\n",
            "118 acc: 0.5987\n",
            "119 loss: 7.62939453125e-05\n",
            "119 acc: 0.6035\n",
            "120 loss: 2.5510787963867188e-05\n",
            "120 acc: 0.6049\n",
            "121 loss: 0.39628517627716064\n",
            "121 acc: 0.6024\n",
            "122 loss: 0.00010085105895996094\n",
            "122 acc: 0.6054\n",
            "123 loss: 0.11513686180114746\n",
            "123 acc: 0.6005\n",
            "124 loss: 0.013750791549682617\n",
            "124 acc: 0.6084\n",
            "125 loss: 0.0006392002105712891\n",
            "125 acc: 0.6025\n",
            "126 loss: 0.00028133392333984375\n",
            "126 acc: 0.6019\n",
            "127 loss: 0.0007103681564331055\n",
            "127 acc: 0.6062\n",
            "128 loss: 0.0001405477523803711\n",
            "128 acc: 0.6098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPxIKGMr5tFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBvJpN0G5tT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}